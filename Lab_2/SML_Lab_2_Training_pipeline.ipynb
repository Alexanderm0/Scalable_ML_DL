{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Libraries installation"
      ],
      "metadata": {
        "id": "6kQ8lR-Qe4l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg"
      ],
      "metadata": {
        "id": "DDbM9_zSdGHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "RuotZVyddKXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub"
      ],
      "metadata": {
        "id": "QwHiJH2rdOZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "oP11nKZ2dQzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, "
      ],
      "metadata": {
        "id": "LeGqjmcRdT0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download the feature store in Gdrive"
      ],
      "metadata": {
        "id": "av6eVOXJK-6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "from googleapiclient.http import MediaFileUpload"
      ],
      "metadata": {
        "id": "IJY3sfu5O7gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5gpZJ2fK6G1"
      },
      "outputs": [],
      "source": [
        "# Search for the file with the given name\n",
        "results = drive_service.files().list(q=\"name='file_train'\").execute()\n",
        "\n",
        "# Get the first file that matches the search query\n",
        "file = results['files'][0]\n",
        "\n",
        "# Print the id of the file\n",
        "print('File ID: {}'.format(file['id']))\n",
        "\n",
        "x = drive_service.files().get(fileId=file['id'], fields=\"*\").execute()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x)"
      ],
      "metadata": {
        "id": "Ifr_aCTeQowP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "request = drive_service.files().get_media(fileId=file['id'])\n",
        "\n",
        "# Use the open() function to create a new file\n",
        "with open(os.path.join(\".\", file[\"name\"]), \"wb\") as fh:\n",
        "    # Use the io.BytesIO object to hold the file's media data\n",
        "    media_data = io.BytesIO()\n",
        "\n",
        "    # Initialize the MediaIoBaseDownload object with the io.BytesIO object\n",
        "    downloader = MediaIoBaseDownload(media_data, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "\n",
        "    # Write the file's media data to the file on disk\n",
        "    fh.write(media_data.getvalue())"
      ],
      "metadata": {
        "id": "YA_ikwP8ShP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = drive_service.files().list(q=\"name='file_test'\").execute()\n",
        "\n",
        "# Get the first file that matches the search query\n",
        "file = results['files'][0]\n",
        "\n",
        "request = drive_service.files().get_media(fileId=file['id'])\n",
        "\n",
        "# Use the open() function to create a new file\n",
        "with open(os.path.join(\".\", file[\"name\"]), \"wb\") as fh:\n",
        "    # Use the io.BytesIO object to hold the file's media data\n",
        "    media_data = io.BytesIO()\n",
        "\n",
        "    # Initialize the MediaIoBaseDownload object with the io.BytesIO object\n",
        "    downloader = MediaIoBaseDownload(media_data, request)\n",
        "\n",
        "    done = False\n",
        "    while done is False:\n",
        "        status, done = downloader.next_chunk()\n",
        "        print(\"Download %d%%.\" % int(status.progress() * 100))\n",
        "\n",
        "    # Write the file's media data to the file on disk\n",
        "    fh.write(media_data.getvalue())"
      ],
      "metadata": {
        "id": "HMxM0QN6YyaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk"
      ],
      "metadata": {
        "id": "IB_ouu1aYTgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = {}\n",
        "common_voice[\"train\"] = load_from_disk(\"file_train\")\n",
        "common_voice[\"test\"] = load_from_disk(\"file_test\")"
      ],
      "metadata": {
        "id": "OLaGhr-pXa1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "5Ltb3YTuNeLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "U1JyBD2bNJL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ],
      "metadata": {
        "id": "_R-owEjYZqKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "Xn1ahRx1Zq0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "HzK68vDOZw9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "LV4Kj2XpZ0UU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ],
      "metadata": {
        "id": "WIJDygyNZ2Yh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/KTH/Scalable"
      ],
      "metadata": {
        "id": "24ZSSuSdZ8kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./improved_whisper_model\",  # change to a repo name of your choice\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=5000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "e7df9LuSZ_0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "w9YUgxzRaKc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "yzc6yH6NaMxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.train(resume_from_checkpoint='./improved_whisper_model/checkpoint-4000')\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Rg4794p6aPOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_11_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: nl, split: test\",\n",
        "    \"language\": \"Dutch\",\n",
        "    \"model_name\": \"/improved_whisper\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-small\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "    \"tags\": \"hf-asr-leaderboard\",\n",
        "}"
      ],
      "metadata": {
        "id": "lrBSlt3NaS9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "jEj8-x28aU2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Upload model to Google Drive\n",
        " This is redundant because we use huggingface to store the model, but could be usefula as backup."
      ],
      "metadata": {
        "id": "_G47TvcAbAHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for file in os.listdir(\"./improved_whisper_model\"):\n",
        "    print(\"UPLOADING\", file)\n",
        "    file_metadata = {\"name\": \"model\"}\n",
        "    media = MediaFileUpload(os.path.join(\"./improved_whisper_model\", file), resumable=True)\n",
        "    drive_service.files().create(\n",
        "        body=file_metadata, media_body=media, fields=\"id\"\n",
        "    ).execute()"
      ],
      "metadata": {
        "id": "xq8wPxmya_tX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
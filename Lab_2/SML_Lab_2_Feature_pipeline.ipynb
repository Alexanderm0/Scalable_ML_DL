{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries Installation"
      ],
      "metadata": {
        "id": "q4cVN2yigDgY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta3FyxLQeIth"
      },
      "outputs": [],
      "source": [
        "!add-apt-repository -y ppa:jonathonf/ffmpeg-4\n",
        "!apt update\n",
        "!apt install -y ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "qHXjDrpKeWza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install huggingface_hub"
      ],
      "metadata": {
        "id": "9JDjLeqreZOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "kO3mWvF0ebQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Features dataset creation and pre-processing"
      ],
      "metadata": {
        "id": "Jyc_ZgmhgLkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"nl\", split=\"train[:40%]+validation[:40%]\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_11_0\", \"nl\", split=\"test[:40%]\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "YA357u6-ednY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "wlFRbBgJegLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "YyJbIxw-ekwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"Dutch\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "q2zVU2NRep_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"Dutch\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "nHDFqqUgetUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "mP5iU5JKevQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "DjaICNDaexOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "Q15urcTAe10Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "\n",
        "    # compute log-Mel input features from input audio array \n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "\n",
        "    # encode target text to label ids \n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "kGtZ-X_se3o-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "795RFXsle68u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save dataset on the disk"
      ],
      "metadata": {
        "id": "8JsfG8Cbfy_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_voice[\"train\"].save_to_disk(\"train\")\n",
        "common_voice[\"test\"].save_to_disk(\"test\")"
      ],
      "metadata": {
        "id": "u8u4KWuJe-nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload features dataset to Google Drive"
      ],
      "metadata": {
        "id": "7ruKBIWVf3NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from googleapiclient.discovery import build\n",
        "drive_service = build('drive', 'v3')\n",
        "from googleapiclient.http import MediaFileUpload"
      ],
      "metadata": {
        "id": "OJwLpzTxf5W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "\n",
        "for file in os.listdir(\"train\"):\n",
        "    print(\"UPLOADING\", file)\n",
        "    file_metadata = {\"name\": \"file_train\"}\n",
        "    media = MediaFileUpload(os.path.join(\"train\", file), resumable=True)\n",
        "    drive_service.files().create(\n",
        "        body=file_metadata, media_body=media, fields=\"id\"\n",
        "    ).execute()\n",
        "    print('File ID: {}'.format(created.get('id')))\n",
        "\n",
        "for file in os.listdir(\"test\"):\n",
        "    print(\"UPLOADING\", file)\n",
        "    file_metadata = {\"name\": \"file_test\"}\n",
        "    media = MediaFileUpload(os.path.join(\"test\", file), resumable=True)\n",
        "    drive_service.files().create(\n",
        "        body=file_metadata, media_body=media, fields=\"id\"\n",
        "    ).execute()\n",
        "    print('File ID: {}'.format(created.get('id')))\n"
      ],
      "metadata": {
        "id": "H1iOD6-egFHh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}